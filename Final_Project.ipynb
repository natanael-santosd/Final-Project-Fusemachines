{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBuG+C7nUY2pBumxQvfYk6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/natanael-santosd/Final-Project-Fusemachines/blob/main/Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **An Example of Emotion Detection and Topic Modeling Analysis of Crime News Comments on Instagram: The Paula Santana's Case**\n",
        "#### By Natanael Santos Delgado"
      ],
      "metadata": {
        "id": "02tUZmmLNLqw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "In this project, I am trying to find out what are the most common reactions of the Dominican users of IG when prompted crime news. I am analyzing the case of Paula Santana, who was the victim of two co-workers (killed in her job); one of them she had reported for harassment, but Human Resources didn't take it seriously. These kind of news always cause a strong reaction from the public, so I think it should be interesting to analyze these comments and try to establish the public's position on how to adress these issues, how it affects security concerns, especially for women, and a reflection on the victim's validation: people care more about victim's that are young, students, beautiful, etc. rather than the nature of the crime.\n",
        "\n",
        "For emotion detection, I use a Support Vector Machine (SVM) classifier trained on TF-IDF vectors of comment text and for topic modeling, I use Latent Dirichlet Allocation (LDA). I also use a Multinomial Naive Bayes classifier to classify comments into sentiment categories such as positive, negative, or neutral or just how strong the negative reactions were.\n",
        "\n",
        "One of the main challenges was the data collection, because web scraping for IG web seems to be very difficult, hence, many researchers prefer to extract comments from Twitter. I decided to use APIFY's instagram scraper for small news and excel cleaning for the mainstream media news. Also, the scope of my research questions might not be answered by the applied methods."
      ],
      "metadata": {
        "id": "ZPFs39PSot5B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Literature Review**\n",
        "\n",
        "## 1.1. Discourse Analysis\n",
        "\n",
        "The International Encyclopedia of Education (2023) defines **discourse analysis** as \"*the epistemological framework for investigating discourse which allows it to approach the variety of discursive genres and to describe the complexity of the discourse and of the interaction*\".\n",
        "\n",
        "Not from me: *Discourse analysis is a field of research composed of multiple heterogeneous, largely qualitative, approaches to the study of relationships between language-in-use and the social world. Researchers in the field typically view language as a form of social practice that influences the social world, and vice versa. Many contemporary varieties of discourse analysis have, explicitly or implicitly, been influenced by Michel Foucault's theories related to power, knowledge, and discourse*.\n",
        "\n",
        "However, discourse analysis has been traditionally a qualitative research approach, that aims to extract social meaning from the study of the use of the language. According to Melissa N.P. Johnson, Ethan McLean (2020)   researchers in the field typically view language as a form of social practice that influences the social world, and vice versa.\n",
        "\n",
        "https://www.sciencedirect.com/science/article/abs/pii/S0747563216305209\n",
        "\n",
        "https://www.sciencedirect.com/science/article/abs/pii/S0010945220301854\n",
        "\n",
        "https://aclanthology.org/P19-4003/\n",
        "\n",
        "\n",
        "Not mine: Discourse analysis has been a fundamental problem in the ACL community, where the focus is to develop tools to automatically model language phenomena that go beyond the individual sentences. With the ongoing neural revolution, as the methods become more effective and flexible, analysis and interpretability beyond the sentence-level\n",
        "is of particular interests for many core language processing tasks like language modeling (Ji et al., 2016) and applications such as machine translation and its evaluation (Sennrich, 2018; Laubli et al., 2018; Joty et al., 2017), text categorization (Ji and Smith, 2017), and sentiment analysis (Nejat et al., 2017). With the advent of Internet technologies, new forms of discourse are emerging (e.g., emails and discussion forums) with novel set of challenges for the computational models.\n",
        "\n",
        "## 1.2 Emotion Detection and Topic Modeling\n",
        "\n",
        "Emotion can be expressed in many ways that can be seen such as facial expression and gestures, speech and by written text. Emotion Detection in text documents is essentially a content - based classification problem involving concepts from the domains of Natural Language Processing as well as Machine Learning.\n",
        "\n",
        "https://arxiv.org/abs/1205.4944\n",
        "\n",
        "## 2."
      ],
      "metadata": {
        "id": "Hf47JgdSrzg2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AGwxqo_1NJZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mainstream Media Coverage\n",
        "\n",
        "**Author:** Listín Diario \\\n",
        "**Date:**"
      ],
      "metadata": {
        "id": "rWCZz1tHCqIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "import demoji\n",
        "import emoji"
      ],
      "metadata": {
        "id": "IIQgHXzXSt7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comment_text = \"Que pena, ahí debe de estar preso hasta el seguridad hasta que todo se aclare y se haga justicia!\"\n",
        "blob = TextBlob(comment_text)\n",
        "sentiment_score = blob.sentiment.polarity\n",
        "print(sentiment_score)"
      ],
      "metadata": {
        "id": "sNoXNTvNCll1",
        "outputId": "f0d7d742-17cd-4bf4-c2ce-10fa743f6ce0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First I want to convert all the emojis into text. This is a source of bias, since I will be defining the meaning, and maybe people did not have that intention or might not be exactly what they meant, but there's comments that only have emojis, so I would not like to delete them."
      ],
      "metadata": {
        "id": "W0039gXhqgob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('diariolibre1.csv')\n",
        "df['data'] = df['data'].astype('str')\n",
        "\n",
        "# Function to extract emojis from a string\n",
        "def extract_distinct_emojis(text):\n",
        "    return emoji.distinct_emoji_list(text)\n",
        "\n",
        "# Apply the function to each value in the 'data' column\n",
        "emojis_list = df['data'].apply(extract_distinct_emojis)\n",
        "\n",
        "# Aggregate all extracted emojis into a single list\n",
        "all_emojis = [emoji for sublist in emojis_list for emoji in sublist]\n",
        "\n",
        "# Obtain unique emojis\n",
        "unique_emojis = list(set(all_emojis))\n",
        "\n",
        "print(\"Unique List of Emojis:\")\n",
        "print(unique_emojis)\n",
        "\n",
        "emoji_mapping = {\n",
        "    '🙌':'',\n",
        "    '🫂':'',\n",
        "    '😩':'',\n",
        "    '😂':'',\n",
        "    '🌹':'',\n",
        "    '🥲':'',\n",
        "    '💜':'',\n",
        "    '☹️':'',\n",
        "    '😳':'',\n",
        "    '🔥':'',\n",
        "    '🖤':'',\n",
        "    '😔':'',\n",
        "    '💰':'',\n",
        "    '👀':'',\n",
        "    '😄':'',\n",
        "    '🙏🏻':'',\n",
        "    '😖':'',\n",
        "    '🇩🇴':'',\n",
        "    '👏':'',\n",
        "    '😞':'',\n",
        "    '🙈':'',\n",
        "    '⚖️':'',\n",
        "    '😒':'',\n",
        "    '\\U0001f979':'',\n",
        "    '👿':'',\n",
        "    '😢':'',\n",
        "    '❤️':'',\n",
        "    '😇':'',\n",
        "    '😮':'',\n",
        "    '🙃':'',\n",
        "    '😦':'',\n",
        "    '🕯':'',\n",
        "    '😍':'',\n",
        "    '🤬':'',\n",
        "    '🕊':'',\n",
        "    '🙊':'',\n",
        "    '😪':'',\n",
        "    '🧑\\u200d🎨':'',\n",
        "    '🙏🏼':'',\n",
        "    '❤':'',\n",
        "    '🌎':'',\n",
        "    '✝️':'',\n",
        "    '😭':'',\n",
        "    '🙏':'',\n",
        "    '🐀':'',\n",
        "    '🎥':'',\n",
        "    '🙉':'',\n",
        "    '🥺':'',\n",
        "    '😡':'',\n",
        "    '🕊️':'',\n",
        "    '😥':'',\n",
        "    '🤔':'',\n",
        "    '👎':'',\n",
        "    '💔':''\n",
        "}\n",
        "\n",
        "def all_emojis(dataset):\n",
        "  \"\"\"Iterates over the dataset and extract all strings that contain\n",
        "     an emoji.\n",
        "  \"\"\"\n",
        "  processed_text = \"\"\n",
        "  for char in dataset:\n",
        "    if char in emoji_mapping:\n",
        "      processed_text += emoji_mapping[char]\n",
        "    else:\n",
        "      processed_text += char\n",
        "  return processed_text\n",
        "\n",
        "df['data'] = df['data'].apply(all_emojis)\n",
        "print(df)\n",
        "\n",
        "df_emoji = pd.read_csv('diariolibre1.csv')\n",
        "df_emoji['data'] = df_emoji['data'].astype('str')\n",
        "\n",
        "emoji_mapping_2 = {\n",
        "    '🙌':'esperanza',\n",
        "    '🫂':'abrazo',\n",
        "    '😩':'angustia',\n",
        "    '😂':'',\n",
        "    '🌹':'rosa',\n",
        "    '🥲':'llorar',\n",
        "    '💜':'esperanza',\n",
        "    '☹️':'llorar',\n",
        "    '😳':'asombro',\n",
        "    '🔥':'',\n",
        "    '🖤':'esperanza',\n",
        "    '😔':'pena',\n",
        "    '💰':'dinero',\n",
        "    '👀':'observar',\n",
        "    '😄':'',\n",
        "    '🙏🏻':'esperanza',\n",
        "    '😖':'pena',\n",
        "    '🇩🇴':'',\n",
        "    '👏':'aplauso',\n",
        "    '😞':'dolor',\n",
        "    '🙈':'',\n",
        "    '⚖️':'justicia',\n",
        "    '😒':'cansancio',\n",
        "    '\\U0001f979':'',\n",
        "    '👿':'enojo',\n",
        "    '😢':'dolor',\n",
        "    '❤️':'esperanza',\n",
        "    '😇':'angel',\n",
        "    '😮':'sorpresa',\n",
        "    '🙃':'enojo',\n",
        "    '😦':'sorpresa',\n",
        "    '🕯':'paz',\n",
        "    '😍':'',\n",
        "    '🤬':'enojo',\n",
        "    '🕊':'descansa en paz',\n",
        "    '🙊':'enojo',\n",
        "    '😪':'dolor',\n",
        "    '🧑\\u200d🎨':'',\n",
        "    '🙏🏼':'orar',\n",
        "    '❤':'esperanza',\n",
        "    '🌎':'mundo',\n",
        "    '✝️':'Dios',\n",
        "    '😭':'llorar',\n",
        "    '🙏':'orar',\n",
        "    '🐀':'raton',\n",
        "    '🎥':'pelicula',\n",
        "    '🙉':'',\n",
        "    '🥺':'llorar',\n",
        "    '😡':'enojo',\n",
        "    '🕊️':'descansa en paz',\n",
        "    '😥':'tristeza',\n",
        "    '🤔':'pensar',\n",
        "    '👎':'disgusto',\n",
        "    '💔':'corazon roto'\n",
        "}\n",
        "\n",
        "def all_emojis_emoji(dataset):\n",
        "  \"\"\"Iterates over the dataset and extract all strings that contain\n",
        "     an emoji.\n",
        "  \"\"\"\n",
        "  processed_text = \"\"\n",
        "  for char in dataset:\n",
        "    if char in emoji_mapping_2:\n",
        "      processed_text += emoji_mapping_2[char]\n",
        "    else:\n",
        "      processed_text += char\n",
        "  return processed_text\n",
        "\n",
        "df_emoji['data'] = df_emoji['data'].apply(all_emojis_emoji)\n",
        "print(df_emoji)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1X48zZp5hwrU",
        "outputId": "8b72c998-174b-4ff9-b9a6-8f10de469faa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique List of Emojis:\n",
            "['🙌', '🫂', '😩', '😂', '🌹', '🥲', '💜', '☹️', '😳', '🔥', '🖤', '😔', '💰', '👀', '😄', '🙏🏻', '😖', '🇩🇴', '👏', '😞', '🙈', '⚖️', '😒', '\\U0001f979', '👿', '😢', '❤️', '😇', '😮', '🙃', '😦', '🕯', '😍', '🤬', '🕊', '🙊', '😪', '🧑\\u200d🎨', '🙏🏼', '❤', '🌎', '✝️', '😭', '🙏', '🐀', '🎥', '🙉', '🥺', '😡', '🕊️', '😥', '🤔', '👎', '💔']\n",
            "      id                                               data\n",
            "0      0  No podemos dejar de hacer sonar el caso, hay q...\n",
            "1      1  Y según la empresa, las cámaras del entorno do...\n",
            "2      2     Dios mio, como apagaron el sueño de esa joven,\n",
            "3      3  A ley de 4 meses paulita para graduarnos y mir...\n",
            "4      4  Que raro que no se han filtrado fotos del acos...\n",
            "..   ...                                                ...\n",
            "495  495  Lo malo de este país que orita suertan.ese hij...\n",
            "496  496  Esa muchacha faja trabajando y pagándose su ca...\n",
            "497  497                             Hay Dios mío que dolor\n",
            "498  498  Yo solo digo que eso está bien turbio, alguien...\n",
            "499  499  Y la justicia de aquí es una basura con un rég...\n",
            "\n",
            "[500 rows x 2 columns]\n",
            "      id                                               data\n",
            "0      0  No podemos dejar de hacer sonar el caso, hay q...\n",
            "1      1  Y según la empresa, las cámaras del entorno do...\n",
            "2      2  dolorDios mio, como apagaron el sueño de esa j...\n",
            "3      3  A ley de 4 meses paulita para graduarnos y mir...\n",
            "4      4  Que raro que no se han filtrado fotos del acos...\n",
            "..   ...                                                ...\n",
            "495  495  Lo malo de este país que orita suertan.ese hij...\n",
            "496  496  Esa muchacha faja trabajando y pagándose su ca...\n",
            "497  497                             Hay Dios mío que dolor\n",
            "498  498  Yo solo digo que eso está bien turbio, alguien...\n",
            "499  499  Y la justicia de aquí es una basura con un rég...\n",
            "\n",
            "[500 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "# df = pd.read_csv('diariolibre1.csv')\n",
        "\n",
        "# Perform sentiment analysis on each comment\n",
        "sentiment_scores = []\n",
        "for index, row in df.iterrows():\n",
        "    comment_text = row['data']\n",
        "    blob = TextBlob(comment_text)\n",
        "    # Specify the language as 'es' for Spanish\n",
        "    sentiment_score = blob.sentiment.polarity\n",
        "    sentiment_scores.append(sentiment_score)\n",
        "\n",
        "# Aggregate sentiment scores to get an overall\n",
        "# sentiment score for the crime news post\n",
        "overall_sentiment_score = sum(sentiment_scores) / len(sentiment_scores)\n",
        "\n",
        "print(\"Overall Sentiment Score:\", overall_sentiment_score)"
      ],
      "metadata": {
        "id": "wEZcUiN9Cq4W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f78d64f-2ca8-4a5a-e9cb-734ac4d77541"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Sentiment Score: -0.011333333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After receiving a close-to-zero overall sentime score, I suspect that the algorithm doesn't work well on this dataset. Hence, I will check different comments individually to judge whether the result is acceptable."
      ],
      "metadata": {
        "id": "9tHKE1pzDvLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This comment transates to: \"What a tragedy, everyone involved should be in jail, even the security officer, until everything it's clear and justice is done!\"\n",
        "comment_text = \"Que pena, ahí debe de estar preso hasta el seguridad hasta que todo se aclare y se haga justicia!\"\n",
        "blob = TextBlob(comment_text)\n",
        "sentiment_score = blob.sentiment.polarity\n",
        "print(sentiment_score)\n",
        "# We get a sentime score of 0.0, showing it's neutral."
      ],
      "metadata": {
        "id": "F2ktGLUpDP-c",
        "outputId": "500aa302-b1a2-4f7c-a19a-9022cb2b1b1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "X = df['data']\n",
        "\n",
        "# Initialize the CountVectorizer\n",
        "count_vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform the data\n",
        "X_count = count_vectorizer.fit_transform(X)\n",
        "\n",
        "# Initialize the LDA model\n",
        "lda_model = LatentDirichletAllocation(n_components=15, random_state=42)  # Assuming 10 topics\n",
        "\n",
        "# Fit the LDA model\n",
        "lda_model.fit(X_count)\n",
        "\n",
        "# Get the top words for each topic\n",
        "feature_names = count_vectorizer.get_feature_names_out()\n",
        "for topic_idx, topic in enumerate(lda_model.components_):\n",
        "    print(f\"Top words for Topic {topic_idx}:\")\n",
        "    top_words_idx = topic.argsort()[:-10 - 1:-1]  # Top 10 words\n",
        "    top_words = [feature_names[i] for i in top_words_idx]\n",
        "    print(top_words)"
      ],
      "metadata": {
        "id": "UkLfe-7EJfoM",
        "outputId": "54390a8d-8091-415d-81dd-155a103814c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top words for Topic 0:\n",
            "['cuando', 'vida', 'cosa', 'nadie', 'mucho', 'aquí', 'solo', 'eso', 'apoyo', 'su']\n",
            "Top words for Topic 1:\n",
            "['que', 'la', 'de', 'en', 'no', 'se', 'alma', 'paz', 'eso', 'los']\n",
            "Top words for Topic 2:\n",
            "['la', 'no', 'empresa', 'de', 'es', 'el', 'vida', 'por', 'esta', 'con']\n",
            "Top words for Topic 3:\n",
            "['nadie', 'la', 'me', 'está', 'lo', 'decir', 'nada', 'sabe', 'que', 'caso']\n",
            "Top words for Topic 4:\n",
            "['que', 'la', 'no', 'justicia', 'de', 'se', 'en', 'dios', 'haga', 'los']\n",
            "Top words for Topic 5:\n",
            "['de', 'que', 'la', 'es', 'una', 'en', 'el', 'lo', 'misericordia', 'esto']\n",
            "Top words for Topic 6:\n",
            "['dios', 'mío', 'pena', 'mio', 'que', 'pobre', 'padre', 'este', 'mundo', 'misericordia']\n",
            "Top words for Topic 7:\n",
            "['el', 'de', 'qué', 'para', 'ojalá', 'culpable', 'un', 'aún', 'sabe', 'pobre']\n",
            "Top words for Topic 8:\n",
            "['que', 'de', 'la', 'los', 'se', 'el', 'no', 'todo', 'un', 'está']\n",
            "Top words for Topic 9:\n",
            "['que', 'de', 'la', 'para', 'por', 'los', 'el', 'lo', 'pena', 'dios']\n",
            "Top words for Topic 10:\n",
            "['que', 'de', 'una', 'no', 'para', 'lo', 'el', 'es', 'esa', 'alma']\n",
            "Top words for Topic 11:\n",
            "['justicia', 'es', 'que', 'una', 'se', 'no', 'el', 'con', 'del', 'yo']\n",
            "Top words for Topic 12:\n",
            "['que', 'no', 'un', 'por', 'si', 'lo', 'la', 'se', 'en', 'empresa']\n",
            "Top words for Topic 13:\n",
            "['de', 'la', 'en', 'para', 'que', 'su', 'no', 'el', 'justicia', 'con']\n",
            "Top words for Topic 14:\n",
            "['que', 'de', 'el', 'la', 'no', 'caso', 'en', 'se', 'le', 'su']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming 'df' is your pandas DataFrame with 'id' and 'data' columns\n",
        "df = pd.read_csv('svm_data.csv')\n",
        "df['data'] = df['data'].astype('str')\n",
        "encoded_df = pd.DataFrame()\n",
        "encoded_df['data'] = df['data']\n",
        "encoded_df['Justice'] = df['Justicia/Pena/Gobierno/Impunidad'] | df['Empresa'] | df['Pobre/Dinero']\n",
        "encoded_df['Victim'] = df['Joven/Muchacha/Chica/Mujer'] | df['Sueño']\n",
        "encoded_df['Harassment'] = df['Acoso']\n",
        "encoded_df['Sad'] = df['Pena/Impotencia']\n",
        "print(encoded_df)\n",
        "#\n",
        "# Load your dataset containing comments and label columns ('Justicia', 'Victim', 'Acoso')\n",
        "# Assuming df is your DataFrame containing comments and label columns\n",
        "# df = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Preprocess comments and extract features\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # You can adjust max_features as needed\n",
        "X = tfidf_vectorizer.fit_transform(df['data'])\n",
        "\n",
        "# Concatenate label columns into a single column for y\n",
        "y = encoded_df[['Justice', 'Victim', 'Harassment','Sad']].astype(str).agg(''.join, axis=1)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the SVM classifier\n",
        "svm_classifier = SVC(kernel='linear')\n",
        "\n",
        "# Train the SVM classifier\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing data\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "vXACtsS5YjFz",
        "outputId": "f5142026-1672-426c-e9f7-6fa5d6f94f93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                  data  Justice  Victim  \\\n",
            "0    No podemos dejar de hacer sonar el caso, hay q...        1       0   \n",
            "1    Y según la empresa, las cámaras del entorno do...        1       0   \n",
            "2     😢Dios mio, como apagaron el sueño de esa joven,😢        0       1   \n",
            "3    A ley de 4 meses paulita para graduarnos y mir...        0       1   \n",
            "4    Que raro que no se han filtrado fotos del acos...        1       0   \n",
            "..                                                 ...      ...     ...   \n",
            "496  Esa muchacha faja trabajando y pagándose su ca...        0       1   \n",
            "497                             Hay Dios mío que dolor        0       0   \n",
            "498  Yo solo digo que eso está bien turbio, alguien...        1       0   \n",
            "499  Y la justicia de aquí es una basura con un rég...        1       0   \n",
            "500                                                nan      127      56   \n",
            "\n",
            "     Harassment  Sad  \n",
            "0             0    0  \n",
            "1             0    0  \n",
            "2             0    0  \n",
            "3             0    0  \n",
            "4             1    0  \n",
            "..          ...  ...  \n",
            "496           0    0  \n",
            "497           0    1  \n",
            "498           0    0  \n",
            "499           0    0  \n",
            "500          20  318  \n",
            "\n",
            "[501 rows x 5 columns]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        0001       0.74      0.97      0.84        59\n",
            "        0010       0.00      0.00      0.00         1\n",
            "        0100       0.50      0.07      0.12        14\n",
            "        1000       0.53      0.53      0.53        19\n",
            "        1010       0.00      0.00      0.00         3\n",
            "        1100       0.67      0.40      0.50         5\n",
            "\n",
            "    accuracy                           0.69       101\n",
            "   macro avg       0.41      0.33      0.33       101\n",
            "weighted avg       0.63      0.69      0.63       101\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class '000': This binary number indicates that all three labels are absent (0) for a given instance. Therefore, this class corresponds to the absence of 'Justice', 'Victim', and 'Harassment'.\n",
        "\n",
        "Class '001': This binary number indicates that 'Acoso' is present (1) while 'Justicia' and 'Victim' are absent (0).\n",
        "\n",
        "Class '010': This binary number indicates that 'Victim' is present (1) while 'Justicia' and 'Acoso' are absent (0).\n",
        "\n",
        "Class '100': This binary number indicates that 'Justice' is present (1) while 'Victim' and 'Acoso' are absent (0).\n",
        "\n",
        "Class '101': This binary number indicates that both 'Justicia' and 'Acoso' are present (1) while 'Victim' is absent (0).\n",
        "\n",
        "Class '110': This binary number indicates that both 'Justicia' and 'Victim' are present (1) while 'Acoso' is absent (0)."
      ],
      "metadata": {
        "id": "6uQrNyKfkuES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import emoji\n",
        "from textblob import TextBlob"
      ],
      "metadata": {
        "id": "IULKzOYUgkRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('diariolibre2.csv')\n",
        "\n",
        "# Perform sentiment analysis on each comment\n",
        "sentiment_scores = []\n",
        "for index, row in df.iterrows():\n",
        "    comment_text = row['data']\n",
        "    blob = TextBlob(comment_text)\n",
        "    sentiment_score = blob.sentiment.polarity\n",
        "    sentiment_scores.append(sentiment_score)\n",
        "\n",
        "# Aggregate sentiment scores to get an overall sentiment score for the crime news post\n",
        "overall_sentiment_score = sum(sentiment_scores) / len(sentiment_scores)\n",
        "\n",
        "print(\"Overall Sentiment Score:\", overall_sentiment_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXgVwq5haZR0",
        "outputId": "504aa91c-b947-446e-ede2-197798b112a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Sentiment Score: -0.010933333333333331\n"
          ]
        }
      ]
    }
  ]
}