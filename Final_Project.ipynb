{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3mjBM8BsmTrrOMFkjdwq5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/natanael-santosd/Final-Project-Fusemachines/blob/main/Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **An Example of Emotion Detection and Topic Modeling Analysis of Crime News Comments on Instagram: The Paula Santana's Case**\n",
        "#### By Natanael Santos Delgado"
      ],
      "metadata": {
        "id": "02tUZmmLNLqw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "In this project, I am trying to find out what are the most common reactions of the Dominican users of IG when prompted crime news. I am analyzing the case of Paula Santana, who was the victim of two co-workers (killed in her job); one of them she had reported for harassment, but Human Resources didn't take it seriously. These kind of news always cause a strong reaction from the public, so I think it should be interesting to analyze these comments and try to establish the public's position on how to adress these issues, how it affects security concerns, especially for women, and a reflection on the victim's validation: people care more about victim's that are young, students, beautiful, etc. rather than the nature of the crime.\n",
        "\n",
        "For emotion detection, I use a Support Vector Machine (SVM) classifier trained on TF-IDF vectors of comment text and for topic modeling, I use Latent Dirichlet Allocation (LDA). I also use a Multinomial Naive Bayes classifier to classify comments into sentiment categories such as positive, negative, or neutral or just how strong the negative reactions were.\n",
        "\n",
        "One of the main challenges was the data collection, because web scraping for IG web seems to be very difficult, hence, many researchers prefer to extract comments from Twitter. I decided to use APIFY's instagram scraper for small news and excel cleaning for the mainstream media news. Also, the scope of my research questions might not be answered by the applied methods."
      ],
      "metadata": {
        "id": "ZPFs39PSot5B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Literature Review**\n",
        "\n",
        "## 1.1. Discourse Analysis\n",
        "\n",
        "The International Encyclopedia of Education (2023) defines **discourse analysis** as \"*the epistemological framework for investigating discourse which allows it to approach the variety of discursive genres and to describe the complexity of the discourse and of the interaction*\".\n",
        "\n",
        "Not from me: *Discourse analysis is a field of research composed of multiple heterogeneous, largely qualitative, approaches to the study of relationships between language-in-use and the social world. Researchers in the field typically view language as a form of social practice that influences the social world, and vice versa. Many contemporary varieties of discourse analysis have, explicitly or implicitly, been influenced by Michel Foucault's theories related to power, knowledge, and discourse*.\n",
        "\n",
        "However, discourse analysis has been traditionally a qualitative research approach, that aims to extract social meaning from the study of the use of the language. According to Melissa N.P. Johnson, Ethan McLean (2020)   researchers in the field typically view language as a form of social practice that influences the social world, and vice versa.\n",
        "\n",
        "https://www.sciencedirect.com/science/article/abs/pii/S0747563216305209\n",
        "\n",
        "https://www.sciencedirect.com/science/article/abs/pii/S0010945220301854\n",
        "\n",
        "https://aclanthology.org/P19-4003/\n",
        "\n",
        "\n",
        "Not mine: Discourse analysis has been a fundamental problem in the ACL community, where the focus is to develop tools to automatically model language phenomena that go beyond the individual sentences. With the ongoing neural revolution, as the methods become more effective and flexible, analysis and interpretability beyond the sentence-level\n",
        "is of particular interests for many core language processing tasks like language modeling (Ji et al., 2016) and applications such as machine translation and its evaluation (Sennrich, 2018; Laubli et al., 2018; Joty et al., 2017), text categorization (Ji and Smith, 2017), and sentiment analysis (Nejat et al., 2017). With the advent of Internet technologies, new forms of discourse are emerging (e.g., emails and discussion forums) with novel set of challenges for the computational models.\n",
        "\n",
        "## 1.2 Emotion Detection and Topic Modeling\n",
        "\n",
        "Emotion can be expressed in many ways that can be seen such as facial expression and gestures, speech and by written text. Emotion Detection in text documents is essentially a content - based classification problem involving concepts from the domains of Natural Language Processing as well as Machine Learning.\n",
        "\n",
        "https://arxiv.org/abs/1205.4944\n",
        "\n",
        "## 2."
      ],
      "metadata": {
        "id": "Hf47JgdSrzg2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AGwxqo_1NJZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mainstream Media Coverage\n",
        "\n",
        "**Author:** ListÃ­n Diario \\\n",
        "**Date:**"
      ],
      "metadata": {
        "id": "rWCZz1tHCqIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "import demoji\n",
        "import emoji"
      ],
      "metadata": {
        "id": "IIQgHXzXSt7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comment_text = \"Que pena, ahÃ­ debe de estar preso hasta el seguridad hasta que todo se aclare y se haga justicia!\"\n",
        "blob = TextBlob(comment_text)\n",
        "sentiment_score = blob.sentiment.polarity\n",
        "print(sentiment_score)"
      ],
      "metadata": {
        "id": "sNoXNTvNCll1",
        "outputId": "f0d7d742-17cd-4bf4-c2ce-10fa743f6ce0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First I want to convert all the emojis into text. This is a source of bias, since I will be defining the meaning, and maybe people did not have that intention or might not be exactly what they meant, but there's comments that only have emojis, so I would not like to delete them."
      ],
      "metadata": {
        "id": "W0039gXhqgob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('diariolibre1.csv')\n",
        "df['data'] = df['data'].astype('str')\n",
        "\n",
        "# Function to extract emojis from a string\n",
        "def extract_distinct_emojis(text):\n",
        "    return emoji.distinct_emoji_list(text)\n",
        "\n",
        "# Apply the function to each value in the 'data' column\n",
        "emojis_list = df['data'].apply(extract_distinct_emojis)\n",
        "\n",
        "# Aggregate all extracted emojis into a single list\n",
        "all_emojis = [emoji for sublist in emojis_list for emoji in sublist]\n",
        "\n",
        "# Obtain unique emojis\n",
        "unique_emojis = list(set(all_emojis))\n",
        "\n",
        "print(\"Unique List of Emojis:\")\n",
        "print(unique_emojis)\n",
        "\n",
        "emoji_mapping = {\n",
        "    'ğŸ™Œ':'',\n",
        "    'ğŸ«‚':'',\n",
        "    'ğŸ˜©':'',\n",
        "    'ğŸ˜‚':'',\n",
        "    'ğŸŒ¹':'',\n",
        "    'ğŸ¥²':'',\n",
        "    'ğŸ’œ':'',\n",
        "    'â˜¹ï¸':'',\n",
        "    'ğŸ˜³':'',\n",
        "    'ğŸ”¥':'',\n",
        "    'ğŸ–¤':'',\n",
        "    'ğŸ˜”':'',\n",
        "    'ğŸ’°':'',\n",
        "    'ğŸ‘€':'',\n",
        "    'ğŸ˜„':'',\n",
        "    'ğŸ™ğŸ»':'',\n",
        "    'ğŸ˜–':'',\n",
        "    'ğŸ‡©ğŸ‡´':'',\n",
        "    'ğŸ‘':'',\n",
        "    'ğŸ˜':'',\n",
        "    'ğŸ™ˆ':'',\n",
        "    'âš–ï¸':'',\n",
        "    'ğŸ˜’':'',\n",
        "    '\\U0001f979':'',\n",
        "    'ğŸ‘¿':'',\n",
        "    'ğŸ˜¢':'',\n",
        "    'â¤ï¸':'',\n",
        "    'ğŸ˜‡':'',\n",
        "    'ğŸ˜®':'',\n",
        "    'ğŸ™ƒ':'',\n",
        "    'ğŸ˜¦':'',\n",
        "    'ğŸ•¯':'',\n",
        "    'ğŸ˜':'',\n",
        "    'ğŸ¤¬':'',\n",
        "    'ğŸ•Š':'',\n",
        "    'ğŸ™Š':'',\n",
        "    'ğŸ˜ª':'',\n",
        "    'ğŸ§‘\\u200dğŸ¨':'',\n",
        "    'ğŸ™ğŸ¼':'',\n",
        "    'â¤':'',\n",
        "    'ğŸŒ':'',\n",
        "    'âœï¸':'',\n",
        "    'ğŸ˜­':'',\n",
        "    'ğŸ™':'',\n",
        "    'ğŸ€':'',\n",
        "    'ğŸ¥':'',\n",
        "    'ğŸ™‰':'',\n",
        "    'ğŸ¥º':'',\n",
        "    'ğŸ˜¡':'',\n",
        "    'ğŸ•Šï¸':'',\n",
        "    'ğŸ˜¥':'',\n",
        "    'ğŸ¤”':'',\n",
        "    'ğŸ‘':'',\n",
        "    'ğŸ’”':''\n",
        "}\n",
        "\n",
        "def all_emojis(dataset):\n",
        "  \"\"\"Iterates over the dataset and extract all strings that contain\n",
        "     an emoji.\n",
        "  \"\"\"\n",
        "  processed_text = \"\"\n",
        "  for char in dataset:\n",
        "    if char in emoji_mapping:\n",
        "      processed_text += emoji_mapping[char]\n",
        "    else:\n",
        "      processed_text += char\n",
        "  return processed_text\n",
        "\n",
        "df['data'] = df['data'].apply(all_emojis)\n",
        "print(df)\n",
        "\n",
        "df_emoji = pd.read_csv('diariolibre1.csv')\n",
        "df_emoji['data'] = df_emoji['data'].astype('str')\n",
        "\n",
        "emoji_mapping_2 = {\n",
        "    'ğŸ™Œ':'esperanza',\n",
        "    'ğŸ«‚':'abrazo',\n",
        "    'ğŸ˜©':'angustia',\n",
        "    'ğŸ˜‚':'',\n",
        "    'ğŸŒ¹':'rosa',\n",
        "    'ğŸ¥²':'llorar',\n",
        "    'ğŸ’œ':'esperanza',\n",
        "    'â˜¹ï¸':'llorar',\n",
        "    'ğŸ˜³':'asombro',\n",
        "    'ğŸ”¥':'',\n",
        "    'ğŸ–¤':'esperanza',\n",
        "    'ğŸ˜”':'pena',\n",
        "    'ğŸ’°':'dinero',\n",
        "    'ğŸ‘€':'observar',\n",
        "    'ğŸ˜„':'',\n",
        "    'ğŸ™ğŸ»':'esperanza',\n",
        "    'ğŸ˜–':'pena',\n",
        "    'ğŸ‡©ğŸ‡´':'',\n",
        "    'ğŸ‘':'aplauso',\n",
        "    'ğŸ˜':'dolor',\n",
        "    'ğŸ™ˆ':'',\n",
        "    'âš–ï¸':'justicia',\n",
        "    'ğŸ˜’':'cansancio',\n",
        "    '\\U0001f979':'',\n",
        "    'ğŸ‘¿':'enojo',\n",
        "    'ğŸ˜¢':'dolor',\n",
        "    'â¤ï¸':'esperanza',\n",
        "    'ğŸ˜‡':'angel',\n",
        "    'ğŸ˜®':'sorpresa',\n",
        "    'ğŸ™ƒ':'enojo',\n",
        "    'ğŸ˜¦':'sorpresa',\n",
        "    'ğŸ•¯':'paz',\n",
        "    'ğŸ˜':'',\n",
        "    'ğŸ¤¬':'enojo',\n",
        "    'ğŸ•Š':'descansa en paz',\n",
        "    'ğŸ™Š':'enojo',\n",
        "    'ğŸ˜ª':'dolor',\n",
        "    'ğŸ§‘\\u200dğŸ¨':'',\n",
        "    'ğŸ™ğŸ¼':'orar',\n",
        "    'â¤':'esperanza',\n",
        "    'ğŸŒ':'mundo',\n",
        "    'âœï¸':'Dios',\n",
        "    'ğŸ˜­':'llorar',\n",
        "    'ğŸ™':'orar',\n",
        "    'ğŸ€':'raton',\n",
        "    'ğŸ¥':'pelicula',\n",
        "    'ğŸ™‰':'',\n",
        "    'ğŸ¥º':'llorar',\n",
        "    'ğŸ˜¡':'enojo',\n",
        "    'ğŸ•Šï¸':'descansa en paz',\n",
        "    'ğŸ˜¥':'tristeza',\n",
        "    'ğŸ¤”':'pensar',\n",
        "    'ğŸ‘':'disgusto',\n",
        "    'ğŸ’”':'corazon roto'\n",
        "}\n",
        "\n",
        "def all_emojis_emoji(dataset):\n",
        "  \"\"\"Iterates over the dataset and extract all strings that contain\n",
        "     an emoji.\n",
        "  \"\"\"\n",
        "  processed_text = \"\"\n",
        "  for char in dataset:\n",
        "    if char in emoji_mapping_2:\n",
        "      processed_text += emoji_mapping_2[char]\n",
        "    else:\n",
        "      processed_text += char\n",
        "  return processed_text\n",
        "\n",
        "df_emoji['data'] = df_emoji['data'].apply(all_emojis_emoji)\n",
        "print(df_emoji)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1X48zZp5hwrU",
        "outputId": "8b72c998-174b-4ff9-b9a6-8f10de469faa"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique List of Emojis:\n",
            "['ğŸ™Œ', 'ğŸ«‚', 'ğŸ˜©', 'ğŸ˜‚', 'ğŸŒ¹', 'ğŸ¥²', 'ğŸ’œ', 'â˜¹ï¸', 'ğŸ˜³', 'ğŸ”¥', 'ğŸ–¤', 'ğŸ˜”', 'ğŸ’°', 'ğŸ‘€', 'ğŸ˜„', 'ğŸ™ğŸ»', 'ğŸ˜–', 'ğŸ‡©ğŸ‡´', 'ğŸ‘', 'ğŸ˜', 'ğŸ™ˆ', 'âš–ï¸', 'ğŸ˜’', '\\U0001f979', 'ğŸ‘¿', 'ğŸ˜¢', 'â¤ï¸', 'ğŸ˜‡', 'ğŸ˜®', 'ğŸ™ƒ', 'ğŸ˜¦', 'ğŸ•¯', 'ğŸ˜', 'ğŸ¤¬', 'ğŸ•Š', 'ğŸ™Š', 'ğŸ˜ª', 'ğŸ§‘\\u200dğŸ¨', 'ğŸ™ğŸ¼', 'â¤', 'ğŸŒ', 'âœï¸', 'ğŸ˜­', 'ğŸ™', 'ğŸ€', 'ğŸ¥', 'ğŸ™‰', 'ğŸ¥º', 'ğŸ˜¡', 'ğŸ•Šï¸', 'ğŸ˜¥', 'ğŸ¤”', 'ğŸ‘', 'ğŸ’”']\n",
            "      id                                               data\n",
            "0      0  No podemos dejar de hacer sonar el caso, hay q...\n",
            "1      1  Y segÃºn la empresa, las cÃ¡maras del entorno do...\n",
            "2      2     Dios mio, como apagaron el sueÃ±o de esa joven,\n",
            "3      3  A ley de 4 meses paulita para graduarnos y mir...\n",
            "4      4  Que raro que no se han filtrado fotos del acos...\n",
            "..   ...                                                ...\n",
            "495  495  Lo malo de este paÃ­s que orita suertan.ese hij...\n",
            "496  496  Esa muchacha faja trabajando y pagÃ¡ndose su ca...\n",
            "497  497                             Hay Dios mÃ­o que dolor\n",
            "498  498  Yo solo digo que eso estÃ¡ bien turbio, alguien...\n",
            "499  499  Y la justicia de aquÃ­ es una basura con un rÃ©g...\n",
            "\n",
            "[500 rows x 2 columns]\n",
            "      id                                               data\n",
            "0      0  No podemos dejar de hacer sonar el caso, hay q...\n",
            "1      1  Y segÃºn la empresa, las cÃ¡maras del entorno do...\n",
            "2      2  dolorDios mio, como apagaron el sueÃ±o de esa j...\n",
            "3      3  A ley de 4 meses paulita para graduarnos y mir...\n",
            "4      4  Que raro que no se han filtrado fotos del acos...\n",
            "..   ...                                                ...\n",
            "495  495  Lo malo de este paÃ­s que orita suertan.ese hij...\n",
            "496  496  Esa muchacha faja trabajando y pagÃ¡ndose su ca...\n",
            "497  497                             Hay Dios mÃ­o que dolor\n",
            "498  498  Yo solo digo que eso estÃ¡ bien turbio, alguien...\n",
            "499  499  Y la justicia de aquÃ­ es una basura con un rÃ©g...\n",
            "\n",
            "[500 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "# df = pd.read_csv('diariolibre1.csv')\n",
        "\n",
        "# Perform sentiment analysis on each comment\n",
        "sentiment_scores = []\n",
        "for index, row in df.iterrows():\n",
        "    comment_text = row['data']\n",
        "    blob = TextBlob(comment_text)\n",
        "    # Specify the language as 'es' for Spanish\n",
        "    sentiment_score = blob.sentiment.polarity\n",
        "    sentiment_scores.append(sentiment_score)\n",
        "\n",
        "# Aggregate sentiment scores to get an overall\n",
        "# sentiment score for the crime news post\n",
        "overall_sentiment_score = sum(sentiment_scores) / len(sentiment_scores)\n",
        "\n",
        "print(\"Overall Sentiment Score:\", overall_sentiment_score)"
      ],
      "metadata": {
        "id": "wEZcUiN9Cq4W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f78d64f-2ca8-4a5a-e9cb-734ac4d77541"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Sentiment Score: -0.011333333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After receiving a close-to-zero overall sentime score, I suspect that the algorithm doesn't work well on this dataset. Hence, I will check different comments individually to judge whether the result is acceptable."
      ],
      "metadata": {
        "id": "9tHKE1pzDvLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This comment transates to: \"What a tragedy, everyone involved should be in jail, even the security officer, until everything it's clear and justice is done!\"\n",
        "comment_text = \"Que pena, ahÃ­ debe de estar preso hasta el seguridad hasta que todo se aclare y se haga justicia!\"\n",
        "blob = TextBlob(comment_text)\n",
        "sentiment_score = blob.sentiment.polarity\n",
        "print(sentiment_score)\n",
        "# We get a sentime score of 0.0, showing it's neutral."
      ],
      "metadata": {
        "id": "F2ktGLUpDP-c",
        "outputId": "500aa302-b1a2-4f7c-a19a-9022cb2b1b1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import emoji\n",
        "from textblob import TextBlob"
      ],
      "metadata": {
        "id": "IULKzOYUgkRu"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('diariolibre2.csv')\n",
        "\n",
        "# Perform sentiment analysis on each comment\n",
        "sentiment_scores = []\n",
        "for index, row in df.iterrows():\n",
        "    comment_text = row['data']\n",
        "    blob = TextBlob(comment_text)\n",
        "    sentiment_score = blob.sentiment.polarity\n",
        "    sentiment_scores.append(sentiment_score)\n",
        "\n",
        "# Aggregate sentiment scores to get an overall sentiment score for the crime news post\n",
        "overall_sentiment_score = sum(sentiment_scores) / len(sentiment_scores)\n",
        "\n",
        "print(\"Overall Sentiment Score:\", overall_sentiment_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXgVwq5haZR0",
        "outputId": "504aa91c-b947-446e-ede2-197798b112a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Sentiment Score: -0.010933333333333331\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First News Article\n",
        "\n",
        "**Author:** Female, Instagram User\n",
        "\n",
        "**Date:**\n",
        "\n",
        "**Link:**\n",
        "\n",
        "**Content:** Comment\n",
        "\n",
        "**Original Language:** Spanish\n",
        "\n",
        "She was being harassed and the company labeled it as a \"bad-taste joke\" #JusticeForPaula\n",
        "\n",
        "Paula's Case brings to the table the issue of work harassment, and with it:\n",
        "\n",
        "- The absence of efficient mechanisms for prevention and monitoring of work harassment\n",
        "\n",
        "- Normalization of harassment, ignoring that is violence and it can scalate quickly\n",
        "\n",
        "- DesvalidaciÃ³n de experiencia de las vÃ­ctimas"
      ],
      "metadata": {
        "id": "wKoHPQ9Eyfwr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "!pip install chardet\n",
        "import chardet\n",
        "with open('data_journalist.csv', 'rb') as f:\n",
        "    encoding = chardet.detect(f.read())['encoding']\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('data_journalist.csv', encoding=encoding)\n",
        "\n",
        "# Separate features (comment text) and labels (emotion categories)\n",
        "X = df['data']  # comment text\n",
        "y = df[['sad', 'anger', 'fear', 'other']]  # emotion labels\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature extraction using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Train a Support Vector Machine (SVM) classifier for each emotion category\n",
        "svm_models = {}\n",
        "for emotion in ['sad', 'anger', 'fear', 'other']:\n",
        "    clf = SVC(kernel='linear')\n",
        "    clf.fit(X_train_tfidf, y_train[emotion])\n",
        "    svm_models[emotion] = clf\n",
        "\n",
        "# Evaluate the models\n",
        "for emotion in ['sad', 'anger', 'fear', 'other']:\n",
        "    y_pred = svm_models[emotion].predict(X_test_tfidf)\n",
        "    print(f\"Emotion: {emotion}\")\n",
        "    print(classification_report(y_test[emotion], y_pred))"
      ],
      "metadata": {
        "id": "LSuTHlPFas9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01a9764a-0ffc-47d0-c25f-e346b99a9e84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Emotion: sad\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.80      0.67         5\n",
            "           1       0.50      0.25      0.33         4\n",
            "\n",
            "    accuracy                           0.56         9\n",
            "   macro avg       0.54      0.53      0.50         9\n",
            "weighted avg       0.54      0.56      0.52         9\n",
            "\n",
            "Emotion: anger\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.25      0.33         4\n",
            "           1       0.57      0.80      0.67         5\n",
            "\n",
            "    accuracy                           0.56         9\n",
            "   macro avg       0.54      0.53      0.50         9\n",
            "weighted avg       0.54      0.56      0.52         9\n",
            "\n",
            "Emotion: fear\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         9\n",
            "\n",
            "    accuracy                           1.00         9\n",
            "   macro avg       1.00      1.00      1.00         9\n",
            "weighted avg       1.00      1.00      1.00         9\n",
            "\n",
            "Emotion: other\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         9\n",
            "\n",
            "    accuracy                           1.00         9\n",
            "   macro avg       1.00      1.00      1.00         9\n",
            "weighted avg       1.00      1.00      1.00         9\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Second News Article\n",
        "\n",
        "**Author:** Female, Instagram User\n",
        "\n",
        "**Date:**\n",
        "\n",
        "**Link:**\n",
        "\n",
        "**Content:** Comment\n",
        "\n",
        "**Original Language:** Spanish"
      ],
      "metadata": {
        "id": "lw_qxvSG5qCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('data_ricardo.csv')\n",
        "\n",
        "# Separate features (comment text) and labels (emotion categories)\n",
        "X = df['data']  # comment text\n",
        "y = df[['agreement', 'life_imp','other']]  # emotion labels\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature extraction using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Train a Support Vector Machine (SVM) classifier for each emotion category\n",
        "svm_models = {}\n",
        "for emotion in ['agreement', 'life_imp','other']:\n",
        "    clf = SVC(kernel='linear')\n",
        "    clf.fit(X_train_tfidf, y_train[emotion])\n",
        "    svm_models[emotion] = clf\n",
        "\n",
        "# Evaluate the models\n",
        "for emotion in ['agreement', 'life_imp','other']:\n",
        "    y_pred = svm_models[emotion].predict(X_test_tfidf)\n",
        "    print(f\"Emotion: {emotion}\")\n",
        "    print(classification_report(y_test[emotion], y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dPLYvi_5sdV",
        "outputId": "8f23f117-1591-45a6-8a93-ff32d4788463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Emotion: agreement\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.50      0.50      0.50         2\n",
            "           1       0.86      0.86      0.86         7\n",
            "\n",
            "    accuracy                           0.78         9\n",
            "   macro avg       0.68      0.68      0.68         9\n",
            "weighted avg       0.78      0.78      0.78         9\n",
            "\n",
            "Emotion: life_imp\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      1.00      0.93         7\n",
            "           1       1.00      0.50      0.67         2\n",
            "\n",
            "    accuracy                           0.89         9\n",
            "   macro avg       0.94      0.75      0.80         9\n",
            "weighted avg       0.90      0.89      0.87         9\n",
            "\n",
            "Emotion: other\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         9\n",
            "\n",
            "    accuracy                           1.00         9\n",
            "   macro avg       1.00      1.00      1.00         9\n",
            "weighted avg       1.00      1.00      1.00         9\n",
            "\n"
          ]
        }
      ]
    }
  ]
}